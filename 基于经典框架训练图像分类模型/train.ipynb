{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 基于经典网络架构训练图像分类模型\n",
    "\n",
    "### 数据预处理\n",
    "- 数据增强：使用torchvision中自带的transform模块自带的功能\n",
    "- 数据预处理：使用torchvision中自带的datasets模块自带的功能\n",
    "- Dataloader模块直接读取batch数据\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 网络模块设置：\n",
    "- 加载预训练模型，torchvision中有很多经典的网络架构，调用起来也是十分方便的，并且可以用别人训练好的权重参数来继续训练，也就是所谓的迁移学习\n",
    "- 需要注意的是别人训练好的任务可能与我们的任务不是我安全一样的，需要吧最后的head层改一改，一般也就是最后的全连接层，改成咋嫩自己的任务\n",
    "- 训练的时候可以全部从头训练，也可以只训练咱们最后的层。因为前几层都是做特征提取的，本事任务目标是一致的，所以可以直接用别人训练好的参数，只训练最后的层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 网络模型保存预测试\n",
    "- 模型保存的时候可以带有选择性， 例如在验证集中如果当前效果好则保存\n",
    "- 读取模型进行实际测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "import imageio\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- torchvision中内置了很多的经典网络架构，可以直接调用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 下载数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据读取和预处理操作\n",
    "\n",
    "### 制作好数据源：\n",
    "- data_transforms中指定了所有图像预处理操作\n",
    "- imageFolder假设所有的文件按文件夹保存好，每个文件下面是同一类的文件，文件夹的名字为他们分类的名字"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据预处理\n",
    "官方提供的数据集需要处理成目录形式如下\n",
    "flower_data\n",
    "-- train\n",
    "  --class1\n",
    "  --class2\n",
    "  ...\n",
    "-- test\n",
    "-- valid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "\n",
    "# 获取lable\n",
    "labels = scipy.io.loadmat('../data/102flowers/imagelabels.mat')  # 该地址为imagelabels.mat的相对地址\n",
    "labels = np.array(labels['labels'][0]) - 1\n",
    "# print(\"labels:\", labels)\n",
    "\n",
    "# 分离train、test、valid\n",
    "setid = scipy.io.loadmat('../data/102flowers/setid.mat')  # 该地址为setid.mat的相对地址\n",
    "\n",
    "validation = np.array(setid['valid'][0]) - 1\n",
    "np.random.shuffle(validation)\n",
    "\n",
    "train = np.array(setid['trnid'][0]) - 1\n",
    "np.random.shuffle(train)\n",
    "\n",
    "test = np.array(setid['tstid'][0]) - 1\n",
    "np.random.shuffle(test)\n",
    "\n",
    "# 把数据导入flower_dir\n",
    "flower_dir = list()\n",
    "\n",
    "for img in os.listdir(\"../data/102flowers/jpg\"):  # 该地址为源数据图片的相对地址\n",
    "    flower_dir.append(os.path.join(\"../data/102flowers/jpg\", img))\n",
    "\n",
    "flower_dir.sort()\n",
    "# print(flower_dir[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# train_dir\n",
    "path_train = \"../data/102flowers/train\"\n",
    "os.makedirs(path_train, exist_ok=True)\n",
    "des_folder_train = path_train  # 该地址可为新建的训练数据集文件夹的相对地址\n",
    "for tid in test:\n",
    "    #打开图片并获取标签\n",
    "    img = Image.open(flower_dir[tid])\n",
    "    # print(img)\n",
    "    # print(flower_dir[tid])\n",
    "    # img = img.resize((256, 256), Image.ANTIALIAS)\n",
    "    lable = labels[tid]\n",
    "    # print(lable)\n",
    "    path = flower_dir[tid]\n",
    "    # print(\"path:\", path)\n",
    "    base_path = os.path.basename(path)\n",
    "    # print(\"base_path:\", base_path)\n",
    "    classes = \"c\" + str(lable)\n",
    "    class_path = os.path.join(des_folder_train, classes)\n",
    "    # 判断结果\n",
    "    if not os.path.exists(class_path):\n",
    "        os.makedirs(class_path)\n",
    "    # print(\"class_path:\", class_path)\n",
    "    despath = os.path.join(class_path, base_path)\n",
    "    # print(\"despath:\", despath)\n",
    "    img.save(despath)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# valid_dir\n",
    "path_valid = \"../data/102flowers/valid\"\n",
    "os.makedirs(path_valid, exist_ok=True)\n",
    "des_folder_validation = path_valid  # 该地址可为新建的训练数据集文件夹的相对地址\n",
    "\n",
    "for tid in validation:\n",
    "    img = Image.open(flower_dir[tid])\n",
    "    # print(flower_dir[tid])\n",
    "    # img = img.resize((256, 256), Image.ANTIALIAS)\n",
    "    lable = labels[tid]\n",
    "    # print(lable)\n",
    "    path = flower_dir[tid]\n",
    "    # print(\"path:\", path)\n",
    "\n",
    "    base_path = os.path.basename(path)\n",
    "    # print(\"base_path:\", base_path)\n",
    "    classes = \"c\" + str(lable)\n",
    "    class_path = os.path.join(des_folder_validation, classes)\n",
    "    # 判断结果\n",
    "    if not os.path.exists(class_path):\n",
    "        os.makedirs(class_path)\n",
    "    # print(\"class_path:\", class_path)\n",
    "    despath = os.path.join(class_path, base_path)\n",
    "    # print(\"despath:\", despath)\n",
    "    img.save(despath)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# test_dir\n",
    "path_test = \"../data/102flowers/test\"\n",
    "os.makedirs(path_test, exist_ok=True)\n",
    "des_folder_test = path_test # 该地址可为新建的训练数据集文件夹的相对地址\n",
    "\n",
    "for tid in train:\n",
    "    img = Image.open(flower_dir[tid])\n",
    "    # print(flower_dir[tid])\n",
    "    # img = img.resize((256, 256), Image.ANTIALIAS)\n",
    "    lable = labels[tid]\n",
    "    # print(lable)\n",
    "    path = flower_dir[tid]\n",
    "    # print(\"path:\", path)\n",
    "    base_path = os.path.basename(path)\n",
    "    # print(\"base_path:\", base_path)\n",
    "    classes = \"c\" + str(lable)\n",
    "    class_path = os.path.join(des_folder_test, classes)\n",
    "    # 判断结果\n",
    "    if not os.path.exists(class_path):\n",
    "        os.makedirs(class_path)\n",
    "    # print(\"class_path:\", class_path)\n",
    "    despath = os.path.join(class_path, base_path)\n",
    "    # print(\"despath:\", despath)\n",
    "    img.save(despath)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train':\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize([96,96]),              # 缩放图片为96*96的大小\n",
    "                transforms.RandomRotation(45),           # 随机旋转45度(-45,45)之间随机选\n",
    "                transforms.CenterCrop(64),               # 从中心裁剪64*64的图片，64是组中图片的大小\n",
    "                transforms.RandomHorizontalFlip(0.5),    # 随机水平翻转\n",
    "                transforms.RandomVerticalFlip(0.5),      # 随机垂直翻转\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "                                                         # 随机改变亮度、对比度、饱和度和色调\n",
    "                transforms.RandomGrayscale(p=0.025),     # 随机将图片转为灰度图\n",
    "                transforms.ToTensor(),                   # 将图片转为Tensor\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 标准化\n",
    "            ]\n",
    "        ),\n",
    "    'valid':\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize([64,64]),                # 缩放图片为96*96的大小\n",
    "                transforms.ToTensor(),                   # 将图片转为Tensor\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                                         # 标准化要和训练集的一样\n",
    "                # 验证集合是否需要做数据增强，取决于训练好模型之后实际的使用场景中是什么情况。\n",
    "                # 例如。如果预测的环境曝光值比较高，就需要我们对验证集做一些数据增强，比如亮度、对比度、饱和度等\n",
    "                # 再比如，如果预测的环境中有很多噪声，就需要我们对验证集做一些数据增强，比如高斯噪声、椒盐噪声等\n",
    "                # 再比如，如果预测的环境中只能看到一部分图像，就需要我们对验证集做一些数据增强，比如裁剪、旋转等\n",
    "            ]\n",
    "        )\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建dataloader， dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "data_dir = '../data/102flowers/'\n",
    "\n",
    "image_datasets = {x: ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=128, shuffle=True,) for x in ['train', 'valid']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "['c0', 'c1', 'c10', 'c100', 'c101', 'c11', 'c12', 'c13', 'c14', 'c15', 'c16', 'c17', 'c18', 'c19', 'c2', 'c20', 'c21', 'c22', 'c23', 'c24', 'c25', 'c26', 'c27', 'c28', 'c29', 'c3', 'c30', 'c31', 'c32', 'c33', 'c34', 'c35', 'c36', 'c37', 'c38', 'c39', 'c4', 'c40', 'c41', 'c42', 'c43', 'c44', 'c45', 'c46', 'c47', 'c48', 'c49', 'c5', 'c50', 'c51', 'c52', 'c53', 'c54', 'c55', 'c56', 'c57', 'c58', 'c59', 'c6', 'c60', 'c61', 'c62', 'c63', 'c64', 'c65', 'c66', 'c67', 'c68', 'c69', 'c7', 'c70', 'c71', 'c72', 'c73', 'c74', 'c75', 'c76', 'c77', 'c78', 'c79', 'c8', 'c80', 'c81', 'c82', 'c83', 'c84', 'c85', 'c86', 'c87', 'c88', 'c89', 'c9', 'c90', 'c91', 'c92', 'c93', 'c94', 'c95', 'c96', 'c97', 'c98', 'c99']\n"
     ]
    }
   ],
   "source": [
    "print(len(class_names))\n",
    "print(class_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-trnid字段:总共有1020列，每10列为一类花卉的图片，每列上的数字代表图片号。\n",
    "-valid字段:总共有1020列，每10列为一类花卉的图片，每列上的数字代表图片号。\n",
    "-tstid字段:总共有6149列，每一类花卉的列数不定，每列上的数字代表图片号。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 6149, 'valid': 1020}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'21': 'fire lily',\n '3': 'canterbury bells',\n '45': 'bolero deep blue',\n '1': 'pink primrose',\n '34': 'mexican aster',\n '27': 'prince of wales feathers',\n '7': 'moon orchid',\n '16': 'globe-flower',\n '25': 'grape hyacinth',\n '26': 'corn poppy',\n '79': 'toad lily',\n '39': 'siam tulip',\n '24': 'red ginger',\n '67': 'spring crocus',\n '35': 'alpine sea holly',\n '32': 'garden phlox',\n '10': 'globe thistle',\n '6': 'tiger lily',\n '93': 'ball moss',\n '33': 'love in the mist',\n '9': 'monkshood',\n '102': 'blackberry lily',\n '14': 'spear thistle',\n '19': 'balloon flower',\n '100': 'blanket flower',\n '13': 'king protea',\n '49': 'oxeye daisy',\n '15': 'yellow iris',\n '61': 'cautleya spicata',\n '31': 'carnation',\n '64': 'silverbush',\n '68': 'bearded iris',\n '63': 'black-eyed susan',\n '69': 'windflower',\n '62': 'japanese anemone',\n '20': 'giant white arum lily',\n '38': 'great masterwort',\n '4': 'sweet pea',\n '86': 'tree mallow',\n '101': 'trumpet creeper',\n '42': 'daffodil',\n '22': 'pincushion flower',\n '2': 'hard-leaved pocket orchid',\n '54': 'sunflower',\n '66': 'osteospermum',\n '70': 'tree poppy',\n '85': 'desert-rose',\n '99': 'bromelia',\n '87': 'magnolia',\n '5': 'english marigold',\n '92': 'bee balm',\n '28': 'stemless gentian',\n '97': 'mallow',\n '57': 'gaura',\n '40': 'lenten rose',\n '47': 'marigold',\n '59': 'orange dahlia',\n '48': 'buttercup',\n '55': 'pelargonium',\n '36': 'ruby-lipped cattleya',\n '91': 'hippeastrum',\n '29': 'artichoke',\n '71': 'gazania',\n '90': 'canna lily',\n '18': 'peruvian lily',\n '98': 'mexican petunia',\n '8': 'bird of paradise',\n '30': 'sweet william',\n '17': 'purple coneflower',\n '52': 'wild pansy',\n '84': 'columbine',\n '12': \"colt's foot\",\n '11': 'snapdragon',\n '96': 'camellia',\n '23': 'fritillary',\n '50': 'common dandelion',\n '44': 'poinsettia',\n '53': 'primula',\n '72': 'azalea',\n '65': 'californian poppy',\n '80': 'anthurium',\n '76': 'morning glory',\n '37': 'cape flower',\n '56': 'bishop of llandaff',\n '60': 'pink-yellow dahlia',\n '82': 'clematis',\n '58': 'geranium',\n '75': 'thorn apple',\n '41': 'barbeton daisy',\n '95': 'bougainvillea',\n '43': 'sword lily',\n '83': 'hibiscus',\n '78': 'lotus lotus',\n '88': 'cyclamen',\n '94': 'foxglove',\n '81': 'frangipani',\n '74': 'rose',\n '89': 'watercress',\n '73': 'water lily',\n '46': 'wallflower',\n '77': 'passion flower',\n '51': 'petunia'}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('../data/102flowers/cat_to_name.json','r') as f:\n",
    "    cat_to_name = json.load(f)\n",
    "cat_to_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载torch提供的预训练模型，这里使用的是resnet18，如果想使用其他的模型，可以在torchvision.models中查看，然后将下面的代码中的resnet18替换为其他模型即可。\n",
    "- 第一次执行会下载模型的文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu==False:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建模型\n",
    "## 模型参数要不要更新\n",
    "- 有时候用人家的模型就一直用了，"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linweicheng/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/linweicheng/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prameters to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# 先冻住全部层\n",
    "def freeze_parameter_requires_grad(model):\n",
    "        for param in model.parameters(): # 遍历每一层，设置为反向传播不更新梯度\n",
    "            param.requires_grad = False\n",
    "\n",
    "# 把模型的输出层改成自己的，微调预训练模型，Linear默认是更新梯度的\n",
    "def initialize_model(\n",
    "        model_name,\n",
    "        num_classes,\n",
    "        feature_extract,\n",
    "        use_pretrained=True):\n",
    "    if model_name == 'resnet':\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "    if feature_extract:\n",
    "        freeze_parameter_requires_grad(model_ft)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes) # 重新修改最后一层的全连接层\n",
    "\n",
    "    input_size = 64\n",
    "    return model_ft, input_size\n",
    "\n",
    "# 构建模型\n",
    "model_name = \"resnet\"\n",
    "use_pretrained=True\n",
    "feature_extract = True\n",
    "\n",
    "model_ft, input_size = initialize_model(model_name, len(class_names), feature_extract, use_pretrained)\n",
    "\n",
    "# 模型加载到什么卡上面计算\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# 模型保存\n",
    "filename = 'model.pth'\n",
    "\n",
    "# 是否训练所有层\n",
    "param_to_update = model_ft.parameters()  #把模型中所有层名字，权重保存下来\n",
    "\n",
    "\n",
    "print(\"Prameters to learn:\")\n",
    "if feature_extract:\n",
    "    param_to_update = []\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            param_to_update.append(param)\n",
    "            print(\"\\t\", name)\n",
    "else:\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\", name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建优化器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "optimizer_ft = optim.Adam(param_to_update, lr=0.001)\n",
    "# 衰减策略\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss() # 损失函数选择交叉熵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练模块\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "def train_model(\n",
    "        model,\n",
    "        dataloaders,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs=25,\n",
    "        filename='model.pth'\n",
    "):\n",
    "    start = time.time()\n",
    "\n",
    "    best_acc = 0                #每次判断实际迭代结果是否有提升，并替换记录最好的一次\n",
    "\n",
    "    model.to(device)            #模型加载到什么卡上面计算\n",
    "\n",
    "    train_acc_history = []      #记录每次迭代的训练集准确率\n",
    "    val_acc_history = []        #记录每次迭代的验证集准确率\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # 学习率\n",
    "    LRs = [(optimizer.param_groups[0]['lr'])]\n",
    "\n",
    "    # 定一个一个变量保存模型的权重，开始的时候先初始化为当前模型的权重\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # 开始迭代\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('\\033[34m——\\033[0m'*50)\n",
    "\n",
    "        # 训练和验证\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train() # 训练模式\n",
    "                print('training')\n",
    "            else:\n",
    "                model.eval() # 验证模式\n",
    "                print('validating')\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 迭代数据, loader中已经确定了batch_size\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 清零\n",
    "                optimizer.zero_grad()\n",
    "                # 前向传播\n",
    "                outputs = model(inputs)\n",
    "                # 计算损失\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, pred = torch.max(outputs, 1)\n",
    "\n",
    "                # 反向传播\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # 计算损失\n",
    "                running_loss += loss.item() * inputs.size(0) # loss.item()是一个数，inputs.size(0)是一个batch_size\n",
    "                running_corrects += torch.sum(pred == labels.data) # pred是一个batch_size的向量，labels.data是一个batch_size的向量\n",
    "\n",
    "            # 计算损失和准确率\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            # 记录每次迭代的训练集准确率和损失\n",
    "            time_elapsed = time.time() - start # 一个epoch的时间\n",
    "            print('\\033[34m{} Loss:\\033[0m {:.4f}||\\033[34mAcc:\\033[0m {:.4f}||\\033[34mTime:\\033[0m {:.0f}m {:.0f}s'.format(\n",
    "                phase, epoch_loss, epoch_acc, time_elapsed // 60, time_elapsed % 60\n",
    "            ))\n",
    "\n",
    "            # 记录最好的那次模型，这一个epoch中最好的那个模型\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                state = {\n",
    "                    'state_dict': model.state_dict(), # 字典的key是层名字，value是层的权重\n",
    "                    'bst_acc': best_acc, # 记录最佳准确率\n",
    "                    'optimizer': optimizer.state_dict(), # 优化器的状态\n",
    "                }\n",
    "                torch.save(state, filename)  # 保存模型\n",
    "\n",
    "            if phase == 'valid':\n",
    "                 val_acc_history.append(epoch_acc)\n",
    "                 val_losses.append(epoch_loss)\n",
    "\n",
    "            if phase == 'train':\n",
    "                 train_acc_history.append(epoch_acc)\n",
    "                 train_losses.append(epoch_loss)\n",
    "\n",
    "        print('optimizer learning rate: {}'.format(optimizer.param_groups[0]['lr']))\n",
    "        LRs.append(optimizer.param_groups[0]['lr'])\n",
    "        print()\n",
    "        scheduler.step() # 更新学习率， 累计到制定好的step_size，就会更新学习率，这里第11个epoch会更新\n",
    "\n",
    "    time_elapsed = time.time()-start\n",
    "    print('\\033[31mTraining complete in\\033[0m {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60\n",
    "    ))\n",
    "    print('\\033[31mBest val Acc:\\033[0m {:4f}'.format(best_acc))\n",
    "\n",
    "    # 加载最好的模型\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc_history, val_acc_history, train_losses, val_losses, LRs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 执行训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\u001B[34m——\u001B[0m\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/49 [00:17<01:42,  2.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model_ft, train_acc_history, val_acc_history, train_losses, val_losses, LRs \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[25], line 60\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, dataloaders, criterion, optimizer, num_epochs, filename)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# 反向传播\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m phase \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 60\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# 计算损失\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model_ft, train_acc_history, val_acc_history, train_losses, val_losses, LRs = train_model(\n",
    "    model_ft,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer_ft,\n",
    "    num_epochs=25,\n",
    "    filename=filename\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 解冻模型，第二轮训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([102, 512]) from checkpoint, the shape in current model is torch.Size([1000, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([102]) from checkpoint, the shape in current model is torch.Size([1000]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m check_point \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     18\u001B[0m best_acc \u001B[38;5;241m=\u001B[39m check_point[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbst_acc\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 19\u001B[0m model_ft \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_ft\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheck_point\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstate_dict\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m model_ft, train_acc_history2, val_acc_history2, train_losses2, val_losses2, LRs2 \u001B[38;5;241m=\u001B[39m train_model(\n\u001B[1;32m     26\u001B[0m     model_ft,\n\u001B[1;32m     27\u001B[0m     dataloaders,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m     filename\u001B[38;5;241m=\u001B[39mfilename\n\u001B[1;32m     32\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   2036\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2037\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2038\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2040\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2042\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([102, 512]) from checkpoint, the shape in current model is torch.Size([1000, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([102]) from checkpoint, the shape in current model is torch.Size([1000])."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# 加载之前训练好的模型\n",
    "model_ft, input_size = initialize_model(model_name, num_classes=102, feature_extract=False, use_pretrained=True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 继续训练所有参数，lr设置的小一点\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 加载之前的权重模型\n",
    "check_point = torch.load('model.pth')\n",
    "best_acc = check_point['bst_acc']\n",
    "model_ft = model_ft.load_state_dict(check_point['state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_ft, train_acc_history2, val_acc_history2, train_losses2, val_losses2, LRs2 = train_model(\n",
    "    model_ft,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=10,\n",
    "    filename=filename\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
